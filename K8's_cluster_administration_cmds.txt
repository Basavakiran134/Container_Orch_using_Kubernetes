labsuser@master:~$ kubectl get nodes
NAME                        STATUS   ROLES                  AGE     VERSION
master.example.com          Ready    control-plane,master   3d22h   v1.23.4
worker-node-1.example.com   Ready    <none>                 3d22h   v1.23.4
worker-node-2.example.com   Ready    <none>                 3d22h   v1.23.4
labsuser@master:~$ kubectl get node -o wide
NAME                        STATUS   ROLES                  AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION    CONTAINER-RUNTIME
master.example.com          Ready    control-plane,master   3d22h   v1.23.4   172.31.55.121   <none>        Ubuntu 20.04.2 LTS   5.11.0-1028-aws   docker://20.10.12
worker-node-1.example.com   Ready    <none>                 3d22h   v1.23.4   172.31.54.194   <none>        Ubuntu 20.04.2 LTS   5.11.0-1028-aws   docker://20.10.12
worker-node-2.example.com   Ready    <none>                 3d22h   v1.23.4   172.31.53.231   <none>        Ubuntu 20.04.2 LTS   5.11.0-1028-aws   docker://20.10.12
labsuser@master:~$ kubectl describe node master.example.com
Name:               master.example.com
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=master.example.com
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.31.55.121/20
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.204.64
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 25 Nov 2022 07:05:59 +0000
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  master.example.com
  AcquireTime:     <unset>
  RenewTime:       Tue, 29 Nov 2022 05:53:45 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Tue, 29 Nov 2022 05:41:19 +0000   Tue, 29 Nov 2022 05:41:19 +0000   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Tue, 29 Nov 2022 05:50:52 +0000   Fri, 25 Nov 2022 07:05:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Tue, 29 Nov 2022 05:50:52 +0000   Fri, 25 Nov 2022 07:05:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Tue, 29 Nov 2022 05:50:52 +0000   Fri, 25 Nov 2022 07:05:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Tue, 29 Nov 2022 05:50:52 +0000   Fri, 25 Nov 2022 07:26:27 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  172.31.55.121
  Hostname:    master.example.com
Capacity:
  cpu:                2
  ephemeral-storage:  25346000Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8039252Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  23358873562
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7936852Ki
  pods:               110
System Info:
  Machine ID:                 ec2b57389aa571afd70e8a944f5b8c05
  System UUID:                ec2ed5d9-cc2c-313c-ccf4-cdfcd4bd295b
  Boot ID:                    5a42c42f-1b18-4bb1-8063-29c4874a6fea
  Kernel Version:             5.11.0-1028-aws
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.12
  Kubelet Version:            v1.23.4
  Kube-Proxy Version:         v1.23.4
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  kube-system                 calico-kube-controllers-7b8458594b-4jx9d      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d22h
  kube-system                 calico-node-bl295                             250m (12%)    0 (0%)      0 (0%)           0 (0%)         3d22h
  kube-system                 coredns-64897985d-lbp8x                       100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     3d22h
  kube-system                 coredns-64897985d-rbsdx                       100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     3d22h
  kube-system                 etcd-master.example.com                       100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         3d22h
  kube-system                 kube-apiserver-master.example.com             250m (12%)    0 (0%)      0 (0%)           0 (0%)         3d22h
  kube-system                 kube-controller-manager-master.example.com    200m (10%)    0 (0%)      0 (0%)           0 (0%)         3d22h
  kube-system                 kube-proxy-sswpq                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d22h
  kube-system                 kube-scheduler-master.example.com             100m (5%)     0 (0%)      0 (0%)           0 (0%)         3d22h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1100m (55%)  0 (0%)
  memory             240Mi (3%)   340Mi (4%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
  Type    Reason                   Age                    From        Message
  ----    ------                   ----                   ----        -------
  Normal  Starting                 3d21h                  kube-proxy  
  Normal  Starting                 13m                    kube-proxy  
  Normal  Starting                 3d21h                  kubelet     Starting kubelet.
  Normal  NodeHasSufficientMemory  3d21h (x8 over 3d21h)  kubelet     Node master.example.com status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3d21h (x8 over 3d21h)  kubelet     Node master.example.com status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3d21h (x7 over 3d21h)  kubelet     Node master.example.com status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  3d21h                  kubelet     Updated Node Allocatable limit across pods
  Normal  Starting                 13m                    kubelet     Starting kubelet.
  Normal  NodeHasSufficientMemory  13m (x8 over 13m)      kubelet     Node master.example.com status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    13m (x8 over 13m)      kubelet     Node master.example.com status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     13m (x7 over 13m)      kubelet     Node master.example.com status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  13m                    kubelet     Updated Node Allocatable limit across pods
labsuser@master:~$ 
labsuser@master:~$ kubectl get ns
NAME              STATUS   AGE
default           Active   3d23h
kube-node-lease   Active   3d23h
kube-public       Active   3d23h
kube-system       Active   3d23h
labsuser@master:~$ kubectl get pods -n kube-system 
NAME                                         READY   STATUS    RESTARTS        AGE
calico-kube-controllers-7b8458594b-4jx9d     1/1     Running   2 (3d22h ago)   3d23h
calico-node-bl295                            1/1     Running   2 (3d22h ago)   3d23h
calico-node-ft9h6                            1/1     Running   2 (3d22h ago)   3d23h
calico-node-pt7xl                            1/1     Running   2 (3d22h ago)   3d23h
coredns-64897985d-lbp8x                      1/1     Running   2 (3d22h ago)   3d23h
coredns-64897985d-rbsdx                      1/1     Running   2 (3d22h ago)   3d23h			kube-system Name space
etcd-master.example.com                      1/1     Running   2 (3d22h ago)   3d23h
kube-apiserver-master.example.com            1/1     Running   2 (3d22h ago)   3d23h
kube-controller-manager-master.example.com   1/1     Running   2 (3d22h ago)   3d23h
kube-proxy-5j4cr                             1/1     Running   2 (3d22h ago)   3d23h
kube-proxy-drkcz                             1/1     Running   2 (3d22h ago)   3d23h
kube-proxy-sswpq                             1/1     Running   2 (3d22h ago)   3d23h
kube-scheduler-master.example.com            1/1     Running   2 (3d22h ago)   3d23h
labsuser@master:~$ 

----------------------------creating the Pod/Container---------------
labsuser@master:~$ 
labsuser@master:~$ kubectl run cont1 --image nginx --port 80
pod/cont1 created
labsuser@master:~$ kubectl get pod cont1
NAME    READY   STATUS    RESTARTS   AGE
cont1   1/1     Running   0          17s
labsuser@master:~$ kubectl get pod | grep cont1
cont1   1/1     Running   0          37s
labsuser@master:~$ kubectl describe pod cont1
Name:         cont1
Namespace:    default
Priority:     0
Node:         worker-node-2.example.com/172.31.53.231
Start Time:   Tue, 29 Nov 2022 07:56:52 +0000
Labels:       run=cont1
Annotations:  cni.projectcalico.org/containerID: 4fbb3a85930bae74a39e7925a8bf09c12ce580f154c1a846b6f27f88be4426d1
              cni.projectcalico.org/podIP: 192.168.232.193/32
              cni.projectcalico.org/podIPs: 192.168.232.193/32
Status:       Running
IP:           192.168.232.193
IPs:
  IP:  192.168.232.193
Containers:
  cont1:
    Container ID:   docker://7e1edd8a05b7a0665d74bb489e228a3eee154437e392d25b91ab5720141ebeb7
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:e209ac2f37c70c1e0e9873a5f7231e91dcd83fdf1178d8ed36c2ec09974210ba
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 29 Nov 2022 07:56:58 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rhc6f (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rhc6f:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  79s   default-scheduler  Successfully assigned default/cont1 to worker-node-2.example.com
  Normal  Pulling    78s   kubelet            Pulling image "nginx"
  Normal  Pulled     73s   kubelet            Successfully pulled image "nginx" in 4.592093825s
  Normal  Created    73s   kubelet            Created container cont1
  Normal  Started    73s   kubelet            Started container cont1
labsuser@master:~$ ^C
labsuser@master:~$ kubectl get pods -A
NAMESPACE     NAME                                         READY   STATUS    RESTARTS        AGE
default       cont1                                        1/1     Running   0               3m34s
kube-system   calico-kube-controllers-7b8458594b-4jx9d     1/1     Running   2 (3d23h ago)   4d
kube-system   calico-node-bl295                            1/1     Running   2 (3d23h ago)   4d
kube-system   calico-node-ft9h6                            1/1     Running   2 (3d23h ago)   4d
kube-system   calico-node-pt7xl                            1/1     Running   2 (3d23h ago)   4d
kube-system   coredns-64897985d-lbp8x                      1/1     Running   2 (3d23h ago)   4d
kube-system   coredns-64897985d-rbsdx                      1/1     Running   2 (3d23h ago)   4d
kube-system   etcd-master.example.com                      1/1     Running   2 (3d23h ago)   4d
kube-system   kube-apiserver-master.example.com            1/1     Running   2 (3d23h ago)   4d
kube-system   kube-controller-manager-master.example.com   1/1     Running   2 (3d23h ago)   4d
kube-system   kube-proxy-5j4cr                             1/1     Running   2 (3d23h ago)   4d
kube-system   kube-proxy-drkcz                             1/1     Running   2 (3d23h ago)   4d
kube-system   kube-proxy-sswpq                             1/1     Running   2 (3d23h ago)   4d
kube-system   kube-scheduler-master.example.com            1/1     Running   2 (3d23h ago)   4d
labsuser@master:~$ 

---------------------------------creating the cluster---------------------------------
labsuser@master:~$ kubectl cluster-info
Kubernetes control plane is running at https://172.31.55.121:6443
CoreDNS is running at https://172.31.55.121:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
labsuser@master:~$ kubectl cluster-info dump >/tmp/dump
labsuser@master:~$ cat /tmp/dump


---------------------how am i provided with the access to the cluster?-----------------------
labsuser@master:~$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://172.31.55.121:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

-------------------------kubernetes files are stored----------------------

labsuser@master:~$ ls /etc/kubernetes
admin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf
labsuser@master:~$ ls /etc/kubernetes/pki
apiserver-etcd-client.crt  apiserver-kubelet-client.crt  apiserver.crt  ca.crt  etcd                front-proxy-ca.key      front-proxy-client.key  sa.pub
apiserver-etcd-client.key  apiserver-kubelet-client.key  apiserver.key  ca.key  front-proxy-ca.crt  front-proxy-client.crt  sa.key
labsuser@master:~$ ls /etc/kubernetes/pki/etcd
ca.crt  ca.key  healthcheck-client.crt  healthcheck-client.key  peer.crt  peer.key  server.crt  server.key
labsuser@master:~$ sudo ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
labsuser@master:~$ sudo ls /etc/kubernetes/manifests/kube-apiserver.yaml
/etc/kubernetes/manifests/kube-apiserver.yaml
labsuser@master:~$ sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.31.55.121:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=172.31.55.121
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.23.14
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 172.31.55.121
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 172.31.55.121
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 172.31.55.121
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/pki
      name: etc-pki
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/pki
      type: DirectoryOrCreate
    name: etc-pki
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}
labsuser@master:~$ 
----------------------Registering the node -----------------------

labsuser@master:~$ cd cluster
bash: cd: cluster: No such file or directory
labsuser@master:~$ mkdir cluster
labsuser@master:~$ cd cluster
labsuser@master:~/cluster$ vi nodereg.json 
labsuser@master:~/cluster$ cat node
cat: node: No such file or directory
labsuser@master:~/cluster$ cat nodereg.json 
{
  "kind": "Node",
  "apiVersion": "v1",
  "metadata": {
        "name": "worker-node-1.example.com",
        "labels": {
        "name": "firstnode"
        }
  }
}

labsuser@master:~/cluster$ kubectl get nodes
NAME                        STATUS   ROLES                  AGE     VERSION
master.example.com          Ready    control-plane,master   4d23h   v1.23.4
worker-node-1.example.com   Ready    <none>                 4d22h   v1.23.4
worker-node-2.example.com   Ready    <none>                 4d22h   v1.23.4
labsuser@master:~/cluster$ kubectl delete node worker-node-1.example.com
node "worker-node-1.example.com" deleted
labsuser@master:~/cluster$ kubectl get nodes
NAME                        STATUS   ROLES                  AGE     VERSION
master.example.com          Ready    control-plane,master   4d23h   v1.23.4
worker-node-2.example.com   Ready    <none>                 4d22h   v1.23.4
labsuser@master:~/cluster$ kubectl apply -f nodereg.json 
node/worker-node-1.example.com created
labsuser@master:~/cluster$ kubectl get nodes
NAME                        STATUS   ROLES                  AGE     VERSION
master.example.com          Ready    control-plane,master   4d23h   v1.23.4
worker-node-1.example.com   Ready    <none>                 8s      v1.23.4
worker-node-2.example.com   Ready    <none>                 4d22h   v1.23.4
labsuser@master:~/cluster$ 



-----------------creating the dashboard-------------
labsuser@master:~/cluster$ kubectl apply -f  https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
labsuser@master:~/cluster$ kubectl get ns
NAME                   STATUS   AGE
default                Active   4d23h
kube-node-lease        Active   4d23h
kube-public            Active   4d23h
kube-system            Active   4d23h
kubernetes-dashboard   Active   21s

labsuser@master:~/cluster$ kubectl get all -n kubernetes-dashboard
NAME                                             READY   STATUS    RESTARTS   AGE
pod/dashboard-metrics-scraper-799d786dbf-qg945   1/1     Running   0          62s
pod/kubernetes-dashboard-546cbc58cd-xs9f6        1/1     Running   0          62s

NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/dashboard-metrics-scraper   ClusterIP   10.109.97.179    <none>        8000/TCP   62s
service/kubernetes-dashboard        ClusterIP   10.103.187.141   <none>        443/TCP    62s

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/dashboard-metrics-scraper   1/1     1            1           62s
deployment.apps/kubernetes-dashboard        1/1     1            1           62s

NAME                                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/dashboard-metrics-scraper-799d786dbf   1         1         1       62s
replicaset.apps/kubernetes-dashboard-546cbc58cd        1         1         1       62s
labsuser@master:~/cluster$ 



--------------------------taking backup for etcd----------------------
labsuser@master:~$ kubectl cluster-info
Kubernetes control plane is running at https://172.31.55.121:6443
CoreDNS is running at https://172.31.55.121:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
labsuser@master:~$ ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
labsuser@master:~$ sudo cat /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.31.55.121:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://172.31.55.121:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --initial-advertise-peer-urls=https://172.31.55.121:2380
    - --initial-cluster=master.example.com=https://172.31.55.121:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://172.31.55.121:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://172.31.55.121:2380
    - --name=master.example.com
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: k8s.gcr.io/etcd:3.5.1-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}
labsuser@master:~$ sudo apt install etcd-client
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following package was automatically installed and is no longer required:
  distro-info
Use 'sudo apt autoremove' to remove it.
The following NEW packages will be installed:
  etcd-client
0 upgraded, 1 newly installed, 0 to remove and 133 not upgraded.
Need to get 4563 kB of archives.
After this operation, 17.2 MB of additional disk space will be used.
Get:1 http://us-west-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 etcd-client amd64 3.2.26+dfsg-6 [4563 kB]
Fetched 4563 kB in 1s (6879 kB/s)      
Selecting previously unselected package etcd-client.
(Reading database ... 263371 files and directories currently installed.)
Preparing to unpack .../etcd-client_3.2.26+dfsg-6_amd64.deb ...
Unpacking etcd-client (3.2.26+dfsg-6) ...
Setting up etcd-client (3.2.26+dfsg-6) ...
Processing triggers for man-db (2.9.1-1) ...
labsuser@master:~$ sudo apt install etcd-client
Reading package lists... Done
Building dependency tree       
Reading state information... Done
etcd-client is already the newest version (3.2.26+dfsg-6).
The following package was automatically installed and is no longer required:
  distro-info
Use 'sudo apt autoremove' to remove it.
0 upgraded, 0 newly installed, 0 to remove and 133 not upgraded.
labsuser@master:~$ etcdctl
NAME:
   etcdctl - A simple command line client for etcd.

WARNING:
   Environment variable ETCDCTL_API is not set; defaults to etcdctl v2.
   Set environment variable ETCDCTL_API=3 to use v3 API or ETCDCTL_API=2 to use v2 API.

USAGE:
   etcdctl [global options] command [command options] [arguments...]

VERSION:
   3.2.26

COMMANDS:
   backup          backup an etcd directory
   cluster-health  check the health of the etcd cluster
   mk              make a new key with a given value
   mkdir           make a new directory
   rm              remove a key or a directory
   rmdir           removes the key if it is an empty directory or a key-value pair
   get             retrieve the value of a key
   ls              retrieve a directory
   set             set the value of a key
   setdir          create a new directory or update an existing directory TTL
   update          update an existing key with a given value
   updatedir       update an existing directory
   watch           watch a key for changes
   exec-watch      watch a key for changes and exec an executable
   member          member add, remove and list subcommands
   user            user add, grant and revoke subcommands
   role            role add, grant and revoke subcommands
   auth            overall auth controls
   help, h         Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --debug                          output cURL commands which can be used to reproduce the request
   --no-sync                        don't synchronize cluster information before sending request
   --output simple, -o simple       output response in the given format (simple, `extended` or `json`) (default: "simple")
   --discovery-srv value, -D value  domain name to query for SRV records describing cluster endpoints
   --insecure-discovery             accept insecure SRV records describing cluster endpoints
   --peers value, -C value          DEPRECATED - "--endpoints" should be used instead
   --endpoint value                 DEPRECATED - "--endpoints" should be used instead
   --endpoints value                a comma-delimited list of machine addresses in the cluster (default: "http://127.0.0.1:2379,http://127.0.0.1:4001")
   --cert-file value                identify HTTPS client using this SSL certificate file
   --key-file value                 identify HTTPS client using this SSL key file
   --ca-file value                  verify certificates of HTTPS-enabled servers using this CA bundle
   --username value, -u value       provide username[:password] and prompt if password is not supplied.
   --timeout value                  connection timeout per request (default: 2s)
   --total-timeout value            timeout for the command execution (except watch) (default: 5s)
   --help, -h                       show help
   --version, -v                    print the version
labsuser@master:~$ 
labsuser@master:~$ sudo chmod a+rw -R /etc/kubernetes/pki
labsuser@master:~$ sudo ETCDCTL_API=3 etcdctl snapshot save etcd_backup.db \
> --endpoints https://172.31.55.121:2379 \
> --cert=/etc/kubernetes/pki/etcd/server.crt \
> --key=/etc/kubernetes/pki/etcd/server.key \
> --cacert=/etc/kubernetes/pki/etcd/ca.crt
Snapshot saved at etcd_backup.db
labsuser@master:~$ sudo ETCDCTL_API=3 etcdctl --write-out=table snapshot status etcd_backup.db \
> --endpoints https://172.31.55.121:2379 \
> --cert=/etc/kubernetes/pki/etcd/server.crt \
> --key=/etc/kubernetes/pki/etcd/server.key \
> --cacert=/etc/kubernetes/pki/etcd/ca.crt
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| faa7817a |    44571 |        939 |     4.5 MB |
+----------+----------+------------+------------+